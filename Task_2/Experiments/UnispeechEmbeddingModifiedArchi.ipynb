{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KESNzYWpkuOx",
        "outputId": "5680dd93-975b-46d5-d612-b3a81e514ea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, UniSpeechModel, UniSpeechConfig\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from moviepy.editor import VideoFileClip\n",
        "import librosa\n",
        "import os\n",
        "import moviepy.editor\n",
        "from pydub import AudioSegment\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import stft\n",
        "from scipy.io import wavfile\n",
        "import scipy.signal as sps\n",
        "import torch.nn as nn\n",
        "import gc\n",
        "import re"
      ],
      "metadata": {
        "id": "fGs-7pKvkyyL"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = AutoProcessor.from_pretrained(\"patrickvonplaten/unispeech-large-1500h-cv-timit\")\n",
        "model_pret = UniSpeechModel.from_pretrained(\"patrickvonplaten/unispeech-large-1500h-cv-timit\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAGg97nDk1cX",
        "outputId": "e30e0601-f937-4ef9-e372-d3143d72391c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Some weights of UniSpeechModel were not initialized from the model checkpoint at patrickvonplaten/unispeech-large-1500h-cv-timit and are newly initialized: ['unispeech.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'unispeech.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_pret)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT73O6AytR1r",
        "outputId": "f60f276d-8aa5-4bdf-de74-17eedc92a5ab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UniSpeechModel(\n",
            "  (feature_extractor): UniSpeechFeatureEncoder(\n",
            "    (conv_layers): ModuleList(\n",
            "      (0): UniSpeechLayerNormConvLayer(\n",
            "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (activation): GELUActivation()\n",
            "      )\n",
            "      (1-4): 4 x UniSpeechLayerNormConvLayer(\n",
            "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (activation): GELUActivation()\n",
            "      )\n",
            "      (5-6): 2 x UniSpeechLayerNormConvLayer(\n",
            "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (activation): GELUActivation()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (feature_projection): UniSpeechFeatureProjection(\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (encoder): UniSpeechEncoderStableLayerNorm(\n",
            "    (pos_conv_embed): UniSpeechPositionalConvEmbedding(\n",
            "      (conv): ParametrizedConv1d(\n",
            "        1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
            "        (parametrizations): ModuleDict(\n",
            "          (weight): ParametrizationList(\n",
            "            (0): _WeightNorm()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (padding): UniSpeechSamePadLayer()\n",
            "      (activation): GELUActivation()\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "    (layers): ModuleList(\n",
            "      (0-23): 24 x UniSpeechEncoderLayerStableLayerNorm(\n",
            "        (attention): UniSpeechAttention(\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): UniSpeechFeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (output_dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "configuration = UniSpeechConfig(vocab_size=31,num_attention_heads=16,hidden_size=1024,conv_bias=True,feat_extract_norm = \"layer\",num_hidden_layers=12,conv_dim=(512, 512, 512, 512),conv_stride=(5, 2, 2, 2), conv_kernel=(10, 3, 3, 3))"
      ],
      "metadata": {
        "id": "z78vOLbhlAbT"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UniSpeechModel(configuration)"
      ],
      "metadata": {
        "id": "xU4IuF-UlP0A"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGdEbLCklcvf",
        "outputId": "dcd868d3-64e9-4d43-b1ed-3c2bc3ccf145"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UniSpeechConfig {\n",
              "  \"activation_dropout\": 0.1,\n",
              "  \"apply_spec_augment\": true,\n",
              "  \"attention_dropout\": 0.1,\n",
              "  \"bos_token_id\": 1,\n",
              "  \"classifier_proj_size\": 256,\n",
              "  \"codevector_dim\": 256,\n",
              "  \"contrastive_logits_temperature\": 0.1,\n",
              "  \"conv_bias\": true,\n",
              "  \"conv_dim\": [\n",
              "    512,\n",
              "    512,\n",
              "    512,\n",
              "    512\n",
              "  ],\n",
              "  \"conv_kernel\": [\n",
              "    10,\n",
              "    3,\n",
              "    3,\n",
              "    3\n",
              "  ],\n",
              "  \"conv_stride\": [\n",
              "    5,\n",
              "    2,\n",
              "    2,\n",
              "    2\n",
              "  ],\n",
              "  \"ctc_loss_reduction\": \"mean\",\n",
              "  \"ctc_zero_infinity\": false,\n",
              "  \"diversity_loss_weight\": 0.1,\n",
              "  \"do_stable_layer_norm\": false,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"feat_extract_activation\": \"gelu\",\n",
              "  \"feat_extract_norm\": \"layer\",\n",
              "  \"feat_proj_dropout\": 0.0,\n",
              "  \"feat_quantizer_dropout\": 0.0,\n",
              "  \"final_dropout\": 0.1,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout\": 0.1,\n",
              "  \"hidden_size\": 1024,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-05,\n",
              "  \"layerdrop\": 0.1,\n",
              "  \"mask_feature_length\": 10,\n",
              "  \"mask_feature_min_masks\": 0,\n",
              "  \"mask_feature_prob\": 0.0,\n",
              "  \"mask_time_length\": 10,\n",
              "  \"mask_time_min_masks\": 2,\n",
              "  \"mask_time_prob\": 0.05,\n",
              "  \"model_type\": \"unispeech\",\n",
              "  \"num_attention_heads\": 16,\n",
              "  \"num_codevector_groups\": 2,\n",
              "  \"num_codevectors_per_group\": 320,\n",
              "  \"num_conv_pos_embedding_groups\": 16,\n",
              "  \"num_conv_pos_embeddings\": 128,\n",
              "  \"num_ctc_classes\": 80,\n",
              "  \"num_feat_extract_layers\": 4,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"num_negatives\": 100,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"proj_codevector_dim\": 256,\n",
              "  \"replace_prob\": 0.5,\n",
              "  \"transformers_version\": \"4.35.2\",\n",
              "  \"use_weighted_layer_sum\": false,\n",
              "  \"vocab_size\": 31\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_pret.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MX7y4Vx-qF2",
        "outputId": "77cd03c4-f9d3-4f68-ec4e-b1933d71bf62"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UniSpeechConfig {\n",
              "  \"_name_or_path\": \"patrickvonplaten/unispeech-large-1500h-cv-timit\",\n",
              "  \"activation_dropout\": 0.0,\n",
              "  \"apply_spec_augment\": true,\n",
              "  \"architectures\": [\n",
              "    \"UniSpeechForCTC\"\n",
              "  ],\n",
              "  \"attention_dropout\": 0.0,\n",
              "  \"bos_token_id\": 1,\n",
              "  \"classifier_proj_size\": 256,\n",
              "  \"codevector_dim\": 768,\n",
              "  \"contrastive_logits_temperature\": 0.1,\n",
              "  \"conv_bias\": true,\n",
              "  \"conv_dim\": [\n",
              "    512,\n",
              "    512,\n",
              "    512,\n",
              "    512,\n",
              "    512,\n",
              "    512,\n",
              "    512\n",
              "  ],\n",
              "  \"conv_kernel\": [\n",
              "    10,\n",
              "    3,\n",
              "    3,\n",
              "    3,\n",
              "    3,\n",
              "    2,\n",
              "    2\n",
              "  ],\n",
              "  \"conv_stride\": [\n",
              "    5,\n",
              "    2,\n",
              "    2,\n",
              "    2,\n",
              "    2,\n",
              "    2,\n",
              "    2\n",
              "  ],\n",
              "  \"ctc_loss_reduction\": \"mean\",\n",
              "  \"ctc_zero_infinity\": false,\n",
              "  \"diversity_loss_weight\": 0.1,\n",
              "  \"do_stable_layer_norm\": true,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"feat_extract_activation\": \"gelu\",\n",
              "  \"feat_extract_dropout\": 0.0,\n",
              "  \"feat_extract_norm\": \"layer\",\n",
              "  \"feat_proj_dropout\": 0.0,\n",
              "  \"feat_quantizer_dropout\": 0.0,\n",
              "  \"final_dropout\": 0.0,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout\": 0.0,\n",
              "  \"hidden_size\": 1024,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 4096,\n",
              "  \"layer_norm_eps\": 1e-05,\n",
              "  \"layerdrop\": 0.0,\n",
              "  \"mask_channel_length\": 10,\n",
              "  \"mask_channel_prob\": 0.0,\n",
              "  \"mask_feature_length\": 10,\n",
              "  \"mask_feature_min_masks\": 0,\n",
              "  \"mask_feature_prob\": 0.0,\n",
              "  \"mask_time_length\": 10,\n",
              "  \"mask_time_min_masks\": 2,\n",
              "  \"mask_time_min_space\": 1,\n",
              "  \"mask_time_prob\": 0.05,\n",
              "  \"model_type\": \"unispeech\",\n",
              "  \"num_attention_heads\": 16,\n",
              "  \"num_codevector_groups\": 2,\n",
              "  \"num_codevectors_per_group\": 320,\n",
              "  \"num_conv_pos_embedding_groups\": 16,\n",
              "  \"num_conv_pos_embeddings\": 128,\n",
              "  \"num_ctc_classes\": 80,\n",
              "  \"num_feat_extract_layers\": 7,\n",
              "  \"num_hidden_layers\": 24,\n",
              "  \"num_negatives\": 100,\n",
              "  \"pad_token_id\": 28,\n",
              "  \"proj_codevector_dim\": 768,\n",
              "  \"replace_prob\": 0.5,\n",
              "  \"tokenizer_class\": \"Wav2Vec2CTCTokenizer\",\n",
              "  \"torch_dtype\": \"float32\",\n",
              "  \"transformers_version\": \"4.35.2\",\n",
              "  \"use_weighted_layer_sum\": false,\n",
              "  \"vocab_size\": 31\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMKMzU2unD4a",
        "outputId": "5fbbb539-5aeb-4245-9012-0a36ac35a687"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UniSpeechModel(\n",
            "  (feature_extractor): UniSpeechFeatureEncoder(\n",
            "    (conv_layers): ModuleList(\n",
            "      (0): UniSpeechLayerNormConvLayer(\n",
            "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (activation): GELUActivation()\n",
            "      )\n",
            "      (1-3): 3 x UniSpeechLayerNormConvLayer(\n",
            "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (activation): GELUActivation()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (feature_projection): UniSpeechFeatureProjection(\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (encoder): UniSpeechEncoder(\n",
            "    (pos_conv_embed): UniSpeechPositionalConvEmbedding(\n",
            "      (conv): ParametrizedConv1d(\n",
            "        1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
            "        (parametrizations): ModuleDict(\n",
            "          (weight): ParametrizationList(\n",
            "            (0): _WeightNorm()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (padding): UniSpeechSamePadLayer()\n",
            "      (activation): GELUActivation()\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (layers): ModuleList(\n",
            "      (0-11): 12 x UniSpeechEncoderLayer(\n",
            "        (attention): UniSpeechAttention(\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): UniSpeechFeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=1024, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=1024, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_params=[]\n",
        "for name,param in model.named_parameters():\n",
        "  model_params.append(name)\n",
        "  print(name)\n",
        "# print(np.array(model_params))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvdVsxCYr7uK",
        "outputId": "48344f93-7ae3-4ad4-e834-ec73b0c2eea9"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "masked_spec_embed\n",
            "feature_extractor.conv_layers.0.conv.weight\n",
            "feature_extractor.conv_layers.0.conv.bias\n",
            "feature_extractor.conv_layers.0.layer_norm.weight\n",
            "feature_extractor.conv_layers.0.layer_norm.bias\n",
            "feature_extractor.conv_layers.1.conv.weight\n",
            "feature_extractor.conv_layers.1.conv.bias\n",
            "feature_extractor.conv_layers.1.layer_norm.weight\n",
            "feature_extractor.conv_layers.1.layer_norm.bias\n",
            "feature_extractor.conv_layers.2.conv.weight\n",
            "feature_extractor.conv_layers.2.conv.bias\n",
            "feature_extractor.conv_layers.2.layer_norm.weight\n",
            "feature_extractor.conv_layers.2.layer_norm.bias\n",
            "feature_extractor.conv_layers.3.conv.weight\n",
            "feature_extractor.conv_layers.3.conv.bias\n",
            "feature_extractor.conv_layers.3.layer_norm.weight\n",
            "feature_extractor.conv_layers.3.layer_norm.bias\n",
            "feature_projection.layer_norm.weight\n",
            "feature_projection.layer_norm.bias\n",
            "feature_projection.projection.weight\n",
            "feature_projection.projection.bias\n",
            "encoder.pos_conv_embed.conv.bias\n",
            "encoder.pos_conv_embed.conv.parametrizations.weight.original0\n",
            "encoder.pos_conv_embed.conv.parametrizations.weight.original1\n",
            "encoder.layer_norm.weight\n",
            "encoder.layer_norm.bias\n",
            "encoder.layers.0.attention.k_proj.weight\n",
            "encoder.layers.0.attention.k_proj.bias\n",
            "encoder.layers.0.attention.v_proj.weight\n",
            "encoder.layers.0.attention.v_proj.bias\n",
            "encoder.layers.0.attention.q_proj.weight\n",
            "encoder.layers.0.attention.q_proj.bias\n",
            "encoder.layers.0.attention.out_proj.weight\n",
            "encoder.layers.0.attention.out_proj.bias\n",
            "encoder.layers.0.layer_norm.weight\n",
            "encoder.layers.0.layer_norm.bias\n",
            "encoder.layers.0.feed_forward.intermediate_dense.weight\n",
            "encoder.layers.0.feed_forward.intermediate_dense.bias\n",
            "encoder.layers.0.feed_forward.output_dense.weight\n",
            "encoder.layers.0.feed_forward.output_dense.bias\n",
            "encoder.layers.0.final_layer_norm.weight\n",
            "encoder.layers.0.final_layer_norm.bias\n",
            "encoder.layers.1.attention.k_proj.weight\n",
            "encoder.layers.1.attention.k_proj.bias\n",
            "encoder.layers.1.attention.v_proj.weight\n",
            "encoder.layers.1.attention.v_proj.bias\n",
            "encoder.layers.1.attention.q_proj.weight\n",
            "encoder.layers.1.attention.q_proj.bias\n",
            "encoder.layers.1.attention.out_proj.weight\n",
            "encoder.layers.1.attention.out_proj.bias\n",
            "encoder.layers.1.layer_norm.weight\n",
            "encoder.layers.1.layer_norm.bias\n",
            "encoder.layers.1.feed_forward.intermediate_dense.weight\n",
            "encoder.layers.1.feed_forward.intermediate_dense.bias\n",
            "encoder.layers.1.feed_forward.output_dense.weight\n",
            "encoder.layers.1.feed_forward.output_dense.bias\n",
            "encoder.layers.1.final_layer_norm.weight\n",
            "encoder.layers.1.final_layer_norm.bias\n",
            "encoder.layers.2.attention.k_proj.weight\n",
            "encoder.layers.2.attention.k_proj.bias\n",
            "encoder.layers.2.attention.v_proj.weight\n",
            "encoder.layers.2.attention.v_proj.bias\n",
            "encoder.layers.2.attention.q_proj.weight\n",
            "encoder.layers.2.attention.q_proj.bias\n",
            "encoder.layers.2.attention.out_proj.weight\n",
            "encoder.layers.2.attention.out_proj.bias\n",
            "encoder.layers.2.layer_norm.weight\n",
            "encoder.layers.2.layer_norm.bias\n",
            "encoder.layers.2.feed_forward.intermediate_dense.weight\n",
            "encoder.layers.2.feed_forward.intermediate_dense.bias\n",
            "encoder.layers.2.feed_forward.output_dense.weight\n",
            "encoder.layers.2.feed_forward.output_dense.bias\n",
            "encoder.layers.2.final_layer_norm.weight\n",
            "encoder.layers.2.final_layer_norm.bias\n",
            "encoder.layers.3.attention.k_proj.weight\n",
            "encoder.layers.3.attention.k_proj.bias\n",
            "encoder.layers.3.attention.v_proj.weight\n",
            "encoder.layers.3.attention.v_proj.bias\n",
            "encoder.layers.3.attention.q_proj.weight\n",
            "encoder.layers.3.attention.q_proj.bias\n",
            "encoder.layers.3.attention.out_proj.weight\n",
            "encoder.layers.3.attention.out_proj.bias\n",
            "encoder.layers.3.layer_norm.weight\n",
            "encoder.layers.3.layer_norm.bias\n",
            "encoder.layers.3.feed_forward.intermediate_dense.weight\n",
            "encoder.layers.3.feed_forward.intermediate_dense.bias\n",
            "encoder.layers.3.feed_forward.output_dense.weight\n",
            "encoder.layers.3.feed_forward.output_dense.bias\n",
            "encoder.layers.3.final_layer_norm.weight\n",
            "encoder.layers.3.final_layer_norm.bias\n",
            "encoder.layers.4.attention.k_proj.weight\n",
            "encoder.layers.4.attention.k_proj.bias\n",
            "encoder.layers.4.attention.v_proj.weight\n",
            "encoder.layers.4.attention.v_proj.bias\n",
            "encoder.layers.4.attention.q_proj.weight\n",
            "encoder.layers.4.attention.q_proj.bias\n",
            "encoder.layers.4.attention.out_proj.weight\n",
            "encoder.layers.4.attention.out_proj.bias\n",
            "encoder.layers.4.layer_norm.weight\n",
            "encoder.layers.4.layer_norm.bias\n",
            "encoder.layers.4.feed_forward.intermediate_dense.weight\n",
            "encoder.layers.4.feed_forward.intermediate_dense.bias\n",
            "encoder.layers.4.feed_forward.output_dense.weight\n",
            "encoder.layers.4.feed_forward.output_dense.bias\n",
            "encoder.layers.4.final_layer_norm.weight\n",
            "encoder.layers.4.final_layer_norm.bias\n",
            "encoder.layers.5.attention.k_proj.weight\n",
            "encoder.layers.5.attention.k_proj.bias\n",
            "encoder.layers.5.attention.v_proj.weight\n",
            "encoder.layers.5.attention.v_proj.bias\n",
            "encoder.layers.5.attention.q_proj.weight\n",
            "encoder.layers.5.attention.q_proj.bias\n",
            "encoder.layers.5.attention.out_proj.weight\n",
            "encoder.layers.5.attention.out_proj.bias\n",
            "encoder.layers.5.layer_norm.weight\n",
            "encoder.layers.5.layer_norm.bias\n",
            "encoder.layers.5.feed_forward.intermediate_dense.weight\n",
            "encoder.layers.5.feed_forward.intermediate_dense.bias\n",
            "encoder.layers.5.feed_forward.output_dense.weight\n",
            "encoder.layers.5.feed_forward.output_dense.bias\n",
            "encoder.layers.5.final_layer_norm.weight\n",
            "encoder.layers.5.final_layer_norm.bias\n",
            "encoder.layers.6.attention.k_proj.weight\n",
            "encoder.layers.6.attention.k_proj.bias\n",
            "encoder.layers.6.attention.v_proj.weight\n",
            "encoder.layers.6.attention.v_proj.bias\n",
            "encoder.layers.6.attention.q_proj.weight\n",
            "encoder.layers.6.attention.q_proj.bias\n",
            "encoder.layers.6.attention.out_proj.weight\n",
            "encoder.layers.6.attention.out_proj.bias\n",
            "encoder.layers.6.layer_norm.weight\n",
            "encoder.layers.6.layer_norm.bias\n",
            "encoder.layers.6.feed_forward.intermediate_dense.weight\n",
            "encoder.layers.6.feed_forward.intermediate_dense.bias\n",
            "encoder.layers.6.feed_forward.output_dense.weight\n",
            "encoder.layers.6.feed_forward.output_dense.bias\n",
            "encoder.layers.6.final_layer_norm.weight\n",
            "encoder.layers.6.final_layer_norm.bias\n",
            "encoder.layers.7.attention.k_proj.weight\n",
            "encoder.layers.7.attention.k_proj.bias\n",
            "encoder.layers.7.attention.v_proj.weight\n",
            "encoder.layers.7.attention.v_proj.bias\n",
            "encoder.layers.7.attention.q_proj.weight\n",
            "encoder.layers.7.attention.q_proj.bias\n",
            "encoder.layers.7.attention.out_proj.weight\n",
            "encoder.layers.7.attention.out_proj.bias\n",
            "encoder.layers.7.layer_norm.weight\n",
            "encoder.layers.7.layer_norm.bias\n",
            "encoder.layers.7.feed_forward.intermediate_dense.weight\n",
            "encoder.layers.7.feed_forward.intermediate_dense.bias\n",
            "encoder.layers.7.feed_forward.output_dense.weight\n",
            "encoder.layers.7.feed_forward.output_dense.bias\n",
            "encoder.layers.7.final_layer_norm.weight\n",
            "encoder.layers.7.final_layer_norm.bias\n",
            "encoder.layers.8.attention.k_proj.weight\n",
            "encoder.layers.8.attention.k_proj.bias\n",
            "encoder.layers.8.attention.v_proj.weight\n",
            "encoder.layers.8.attention.v_proj.bias\n",
            "encoder.layers.8.attention.q_proj.weight\n",
            "encoder.layers.8.attention.q_proj.bias\n",
            "encoder.layers.8.attention.out_proj.weight\n",
            "encoder.layers.8.attention.out_proj.bias\n",
            "encoder.layers.8.layer_norm.weight\n",
            "encoder.layers.8.layer_norm.bias\n",
            "encoder.layers.8.feed_forward.intermediate_dense.weight\n",
            "encoder.layers.8.feed_forward.intermediate_dense.bias\n",
            "encoder.layers.8.feed_forward.output_dense.weight\n",
            "encoder.layers.8.feed_forward.output_dense.bias\n",
            "encoder.layers.8.final_layer_norm.weight\n",
            "encoder.layers.8.final_layer_norm.bias\n",
            "encoder.layers.9.attention.k_proj.weight\n",
            "encoder.layers.9.attention.k_proj.bias\n",
            "encoder.layers.9.attention.v_proj.weight\n",
            "encoder.layers.9.attention.v_proj.bias\n",
            "encoder.layers.9.attention.q_proj.weight\n",
            "encoder.layers.9.attention.q_proj.bias\n",
            "encoder.layers.9.attention.out_proj.weight\n",
            "encoder.layers.9.attention.out_proj.bias\n",
            "encoder.layers.9.layer_norm.weight\n",
            "encoder.layers.9.layer_norm.bias\n",
            "encoder.layers.9.feed_forward.intermediate_dense.weight\n",
            "encoder.layers.9.feed_forward.intermediate_dense.bias\n",
            "encoder.layers.9.feed_forward.output_dense.weight\n",
            "encoder.layers.9.feed_forward.output_dense.bias\n",
            "encoder.layers.9.final_layer_norm.weight\n",
            "encoder.layers.9.final_layer_norm.bias\n",
            "encoder.layers.10.attention.k_proj.weight\n",
            "encoder.layers.10.attention.k_proj.bias\n",
            "encoder.layers.10.attention.v_proj.weight\n",
            "encoder.layers.10.attention.v_proj.bias\n",
            "encoder.layers.10.attention.q_proj.weight\n",
            "encoder.layers.10.attention.q_proj.bias\n",
            "encoder.layers.10.attention.out_proj.weight\n",
            "encoder.layers.10.attention.out_proj.bias\n",
            "encoder.layers.10.layer_norm.weight\n",
            "encoder.layers.10.layer_norm.bias\n",
            "encoder.layers.10.feed_forward.intermediate_dense.weight\n",
            "encoder.layers.10.feed_forward.intermediate_dense.bias\n",
            "encoder.layers.10.feed_forward.output_dense.weight\n",
            "encoder.layers.10.feed_forward.output_dense.bias\n",
            "encoder.layers.10.final_layer_norm.weight\n",
            "encoder.layers.10.final_layer_norm.bias\n",
            "encoder.layers.11.attention.k_proj.weight\n",
            "encoder.layers.11.attention.k_proj.bias\n",
            "encoder.layers.11.attention.v_proj.weight\n",
            "encoder.layers.11.attention.v_proj.bias\n",
            "encoder.layers.11.attention.q_proj.weight\n",
            "encoder.layers.11.attention.q_proj.bias\n",
            "encoder.layers.11.attention.out_proj.weight\n",
            "encoder.layers.11.attention.out_proj.bias\n",
            "encoder.layers.11.layer_norm.weight\n",
            "encoder.layers.11.layer_norm.bias\n",
            "encoder.layers.11.feed_forward.intermediate_dense.weight\n",
            "encoder.layers.11.feed_forward.intermediate_dense.bias\n",
            "encoder.layers.11.feed_forward.output_dense.weight\n",
            "encoder.layers.11.feed_forward.output_dense.bias\n",
            "encoder.layers.11.final_layer_norm.weight\n",
            "encoder.layers.11.final_layer_norm.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.feature_projection.layer_norm.weight.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kaDrFhUyFN4",
        "outputId": "1c1be526-95d1-42ee-b37a-a21914aeebb8"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([512])"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dic = {}\n",
        "with torch.no_grad():\n",
        "  for name,param in model_pret.named_parameters():\n",
        "    dic[name] = param"
      ],
      "metadata": {
        "id": "zJ51VuD5zVEq"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.feature_extractor.conv_layers[0].conv.weight = dic['feature_extractor.conv_layers.0.conv.weight']\n",
        "model.feature_extractor.conv_layers[0].conv.bias = dic['feature_extractor.conv_layers.0.conv.bias']\n",
        "model.feature_extractor.conv_layers[0].layer_norm.weight = dic['feature_extractor.conv_layers.0.layer_norm.weight']\n",
        "model.feature_extractor.conv_layers[0].layer_norm.bias = dic['feature_extractor.conv_layers.0.layer_norm.bias']\n",
        "\n",
        "model.feature_extractor.conv_layers[1].conv.weight = dic['feature_extractor.conv_layers.1.conv.weight']\n",
        "model.feature_extractor.conv_layers[1].conv.bias = dic['feature_extractor.conv_layers.1.conv.bias']\n",
        "model.feature_extractor.conv_layers[1].layer_norm.weight = dic['feature_extractor.conv_layers.1.layer_norm.weight']\n",
        "model.feature_extractor.conv_layers[1].layer_norm.bias = dic['feature_extractor.conv_layers.1.layer_norm.bias']\n",
        "\n",
        "model.feature_extractor.conv_layers[2].conv.weight = dic['feature_extractor.conv_layers.2.conv.weight']\n",
        "model.feature_extractor.conv_layers[2].conv.bias = dic['feature_extractor.conv_layers.2.conv.bias']\n",
        "model.feature_extractor.conv_layers[2].layer_norm.weight = dic['feature_extractor.conv_layers.2.layer_norm.weight']\n",
        "model.feature_extractor.conv_layers[2].layer_norm.bias = dic['feature_extractor.conv_layers.2.layer_norm.bias']\n",
        "\n",
        "model.feature_extractor.conv_layers[3].conv.weight = dic['feature_extractor.conv_layers.3.conv.weight']\n",
        "model.feature_extractor.conv_layers[3].conv.bias = dic['feature_extractor.conv_layers.3.conv.bias']\n",
        "model.feature_extractor.conv_layers[3].layer_norm.weight = dic['feature_extractor.conv_layers.3.layer_norm.weight']\n",
        "model.feature_extractor.conv_layers[3].layer_norm.bias = dic['feature_extractor.conv_layers.3.layer_norm.bias']"
      ],
      "metadata": {
        "id": "zUtgXtow2Fta"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.feature_projection.layer_norm.weight = dic['feature_projection.layer_norm.weight']\n",
        "model.feature_projection.layer_norm.bias = dic['feature_projection.layer_norm.bias']\n",
        "model.feature_projection.projection.weight = dic['feature_projection.projection.weight']\n",
        "model.feature_projection.projection.bias = dic['feature_projection.projection.bias']"
      ],
      "metadata": {
        "id": "r_WgoO4Z3B6z"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic['encoder.pos_conv_embed.conv.parametrizations.weight.original0'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPgY53_u31om",
        "outputId": "2a37ed88-4660-45f3-b40c-ed38104f6e4e"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.encoder.pos_conv_embed.conv.bias = dic['encoder.pos_conv_embed.conv.bias']\n",
        "model.encoder.pos_conv_embed.conv.parametrizations.weight.original0 = dic['encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
        "model.encoder.pos_conv_embed.conv.parametrizations.weight.original1 = dic['encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
        "model.encoder.layer_norm.weight = dic['encoder.layer_norm.weight']\n",
        "model.encoder.layer_norm.bias = dic['encoder.layer_norm.bias']\n",
        "\n",
        "for i in range(12):\n",
        "  model.encoder.layers[i].attention.k_proj.weight = dic['encoder.layers.%d.attention.k_proj.weight'%i]\n",
        "  model.encoder.layers[i].attention.k_proj.bias = dic['encoder.layers.%d.attention.k_proj.bias'%i]\n",
        "  model.encoder.layers[i].attention.v_proj.weight = dic['encoder.layers.%d.attention.v_proj.weight'%i]\n",
        "  model.encoder.layers[i].attention.v_proj.bias = dic['encoder.layers.%d.attention.v_proj.bias'%i]\n",
        "  model.encoder.layers[i].attention.q_proj.weight = dic['encoder.layers.%d.attention.q_proj.weight'%i]\n",
        "  model.encoder.layers[i].attention.q_proj.bias = dic['encoder.layers.%d.attention.q_proj.bias'%i]\n",
        "  model.encoder.layers[i].attention.out_proj.weight = dic['encoder.layers.%d.attention.out_proj.weight'%i]\n",
        "  model.encoder.layers[i].attention.out_proj.bias = dic['encoder.layers.%d.attention.out_proj.bias'%i]\n",
        "  model.encoder.layers[i].layer_norm.weight = dic['encoder.layers.%d.layer_norm.weight'%i]\n",
        "  model.encoder.layers[i].layer_norm.bias = dic['encoder.layers.%d.layer_norm.bias'%i]\n",
        "  model.encoder.layers[i].feed_forward.intermediate_dense.weight = dic['encoder.layers.%d.feed_forward.intermediate_dense.weight'%i]\n",
        "  model.encoder.layers[i].feed_forward.intermediate_dense.bias = dic['encoder.layers.%d.feed_forward.intermediate_dense.bias'%i]\n",
        "  model.encoder.layers[i].feed_forward.output_dense.weight = dic['encoder.layers.%d.feed_forward.output_dense.weight'%i]\n",
        "  model.encoder.layers[i].feed_forward.output_dense.bias = dic['encoder.layers.%d.feed_forward.output_dense.bias'%i]\n",
        "  model.encoder.layers[i].final_layer_norm.weight = dic['encoder.layers.%d.final_layer_norm.weight'%i]\n",
        "  model.encoder.layers[i].final_layer_norm.bias = dic['encoder.layers.%d.final_layer_norm.bias'%i]"
      ],
      "metadata": {
        "id": "hEAyzZT-3qaB"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(12):\n",
        "  print(torch.mean(model.encoder.layers[i].final_layer_norm.bias - model_pret.encoder.layers[i].final_layer_norm.bias))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b58UOEt6XnJ",
        "outputId": "b97297b5-b05f-48bf-a781-9547e839ee0f"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0., grad_fn=<MeanBackward0>)\n",
            "tensor(0., grad_fn=<MeanBackward0>)\n",
            "tensor(0., grad_fn=<MeanBackward0>)\n",
            "tensor(0., grad_fn=<MeanBackward0>)\n",
            "tensor(0., grad_fn=<MeanBackward0>)\n",
            "tensor(0., grad_fn=<MeanBackward0>)\n",
            "tensor(0., grad_fn=<MeanBackward0>)\n",
            "tensor(0., grad_fn=<MeanBackward0>)\n",
            "tensor(0., grad_fn=<MeanBackward0>)\n",
            "tensor(0., grad_fn=<MeanBackward0>)\n",
            "tensor(0., grad_fn=<MeanBackward0>)\n",
            "tensor(0., grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrDGgUqnpjWy",
        "outputId": "9b207e4e-1eb6-4fc3-ce55-7e972d87fa05"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UniSpeechModel(\n",
              "  (feature_extractor): UniSpeechFeatureEncoder(\n",
              "    (conv_layers): ModuleList(\n",
              "      (0): UniSpeechLayerNormConvLayer(\n",
              "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (activation): GELUActivation()\n",
              "      )\n",
              "      (1-3): 3 x UniSpeechLayerNormConvLayer(\n",
              "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (activation): GELUActivation()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (feature_projection): UniSpeechFeatureProjection(\n",
              "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
              "    (dropout): Dropout(p=0.0, inplace=False)\n",
              "  )\n",
              "  (encoder): UniSpeechEncoder(\n",
              "    (pos_conv_embed): UniSpeechPositionalConvEmbedding(\n",
              "      (conv): ParametrizedConv1d(\n",
              "        1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
              "        (parametrizations): ModuleDict(\n",
              "          (weight): ParametrizationList(\n",
              "            (0): _WeightNorm()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (padding): UniSpeechSamePadLayer()\n",
              "      (activation): GELUActivation()\n",
              "    )\n",
              "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-11): 12 x UniSpeechEncoderLayer(\n",
              "        (attention): UniSpeechAttention(\n",
              "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): UniSpeechFeedForward(\n",
              "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (intermediate_dense): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "          (output_dense): Linear(in_features=3072, out_features=1024, bias=True)\n",
              "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inputting the audio now"
      ],
      "metadata": {
        "id": "qBRxkp-ouw62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_video(url, save_path):\n",
        "    response = requests.get(url, stream=True)\n",
        "    if response.status_code == 200:\n",
        "        with open(save_path, 'wb') as file:\n",
        "            for chunk in response.iter_content(chunk_size=1024):\n",
        "                file.write(chunk)\n",
        "        print(f\"Video downloaded successfully to {save_path}\")\n",
        "    else:\n",
        "        print(f\"Failed to download video. Status code: {response.status_code}\")"
      ],
      "metadata": {
        "id": "JKnY-bHgrEHq"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_file(file_path):\n",
        "    try:\n",
        "        os.remove(file_path)\n",
        "        print(f\"File '{file_path}' successfully deleted.\")\n",
        "    except OSError as e:\n",
        "        print(f\"Error deleting file '{file_path}': {e}\")"
      ],
      "metadata": {
        "id": "fKAk500hu3s_"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mp3_to_waveform(mp3_path):\n",
        "    audio = AudioSegment.from_file(mp3_path, format=\"mp3\").set_frame_rate(16000)\n",
        "    if audio.channels > 1:\n",
        "        audio = audio.set_channels(1)\n",
        "    waveform = np.array(audio.get_array_of_samples())\n",
        "    return waveform, audio.frame_rate"
      ],
      "metadata": {
        "id": "NH64aMyTu5PW"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getWaveform(vid_url):\n",
        "    download_video(vid_url, \"downloaded_video.mp4\")\n",
        "    video = moviepy.editor.VideoFileClip(\"downloaded_video.mp4\")\n",
        "    audio = video.audio\n",
        "    audio.write_audiofile(\"downloaded_audio.mp3\")\n",
        "    delete_file(\"downloaded_video.mp4\")\n",
        "    waveform,sample_rate = mp3_to_waveform(\"downloaded_audio.mp3\")\n",
        "    print(\"Shape of waveform array:\", waveform.shape)\n",
        "    delete_file(\"downloaded_audio.mp3\")\n",
        "    return waveform,sample_rate"
      ],
      "metadata": {
        "id": "fM4HjwyRu6bU"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wave,sample_rate = getWaveform(\"https://video.twimg.com/amplify_video/1019341984543268866/vid/640x360/ewrbY6gdakX6772y.mp4?tag=3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAnBYg-qu7jw",
        "outputId": "b82d7915-7d35-4325-d2fa-15a0e4579f6a"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video downloaded successfully to downloaded_video.mp4\n",
            "MoviePy - Writing audio in downloaded_audio.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "File 'downloaded_video.mp4' successfully deleted.\n",
            "Shape of waveform array: (924320,)\n",
            "File 'downloaded_audio.mp3' successfully deleted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wave.shape,sample_rate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZ5t-0SwvAKK",
        "outputId": "1041b65e-2caa-467a-eb49-2d88e8fcbd09"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((924320,), 16000)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(wave, sampling_rate=sample_rate, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "MicxFKtfvBtC"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs['input_values']=inputs['input_values'].float()\n",
        "inputs['input_values']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA4tjiYevCoz",
        "outputId": "a172d614-25cb-483f-cd96-cf11cbf94a05"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0006, 0.0006, 0.0006,  ..., 0.4543, 0.3841, 0.2471]])"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TANDavSvEp-",
        "outputId": "7262ac63-eaf4-487f-bee1-769b45f22c28"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1035"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zuE99hYvG0R",
        "outputId": "32f04838-4c21-4568-da85-aefd2b03b62f"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_values': tensor([[0.0006, 0.0006, 0.0006,  ..., 0.4543, 0.3841, 0.2471]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], dtype=torch.int32)}"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "last_hidden_states = outputs.last_hidden_state\n",
        "last_hidden_states.shape"
      ],
      "metadata": {
        "id": "7qAUYuJ-vH4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model_pret(**inputs)\n",
        "\n",
        "last_hidden_states = outputs.last_hidden_state\n",
        "last_hidden_states.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "UVVMPMcFvMVV",
        "outputId": "2c866e49-0d2c-48ef-b3fb-fc6755150f41"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-15cc41aa282a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_pret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlast_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlast_hidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kyclqv7Yvgt9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}